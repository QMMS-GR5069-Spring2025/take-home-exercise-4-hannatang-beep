{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd168358-f496-47ab-a22d-681f3c8827bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import config\n",
    "import mysql.connector\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "35b2b148-9ace-4c7c-a9df-46e229dabbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7e7de4d7-040b-43d4-bdff-fa5c35a29342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the raw lap times, pit stops, and driver data\n",
    "df_laps = spark.read.csv(\"s3://columbia-gr5069-main/raw/lap_times.csv\", header=True, inferSchema=True)\n",
    "df_pit = spark.read.csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\", header=True, inferSchema=True)\n",
    "df_drivers = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display schema and a sample of the lap times data\n",
    "df_laps.printSchema()\n",
    "df_laps.select(\"raceId\", \"driverId\", \"lap\", \"position\", \"milliseconds\").show(5)\n",
    "\n",
    "# Count the number of pit stops for each driver in each race\n",
    "df_pit_grouped = df_pit.groupBy(\"raceId\", \"driverId\").agg(count(\"*\").alias(\"pit_stop_count\"))\n",
    "\n",
    "# Join pit stop count back to the lap time dataset\n",
    "df_laps_joined = df_laps.join(df_pit_grouped, on=[\"raceId\", \"driverId\"], how=\"left\").fillna(0)\n",
    "\n",
    "# Select relevant features and cast them to appropriate types\n",
    "df_laps_feat = df_laps_joined.select(\n",
    "    col(\"milliseconds\").cast(DoubleType()),       # target variable\n",
    "    col(\"lap\").cast(IntegerType()),               # lap number\n",
    "    col(\"position\").cast(IntegerType()),          # race position\n",
    "    col(\"pit_stop_count\").cast(IntegerType())     # total pit stops\n",
    ").na.drop()\n",
    "\n",
    "# Preview final dataset used for modeling\n",
    "df_laps_feat.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "03ffc51a-31c6-4c4c-9be3-6218cb757d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "nc -vz ht2668-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com 3306\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f3d9216-e5f4-459e-a1b2-b45d06491a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[20 pts] Create two (2) new tables in your own database where you'll store the predictions from each model for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0a92159c-1ab9-4458-bd33-06094c726a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add an ID column to the prediction DataFrame\n",
    "predDF_with_id = predDF.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Select only the columns needed (id and prediction), NOT include 'features'\n",
    "predDF_clean = predDF_with_id.select(\"id\", \"prediction\")\n",
    "\n",
    "# Connect to RDS and create the database if it doesn't exist\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"ht2668-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com\",\n",
    "    user=\"ht2668_gr5069\",\n",
    "    password=config.MYSQL_PASSWORD\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS gr5069\") \n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# the cleaned DataFrame into MySQL database\n",
    "predDF_clean.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://ht2668-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\",\n",
    "    driver=\"com.mysql.jdbc.Driver\",\n",
    "    dbtable=\"lr_predictions_f1\",   \n",
    "    user=\"ht2668_gr5069\",\n",
    "    password=config.MYSQL_PASSWORD\n",
    ").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f318af85-f321-4a31-b130-75e96ba55b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"milliseconds\", numTrees=100)\n",
    "rfModel = rf.fit(trainDF)\n",
    "\n",
    "# Predict on test set\n",
    "predDF_rf = rfModel.transform(testDF)\n",
    "\n",
    "# Add an ID column to Random Forest prediction DataFrame\n",
    "predDF_rf_with_id = predDF_rf.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Select only id and prediction columns\n",
    "predDF_rf_clean = predDF_rf_with_id.select(\"id\", \"prediction\")\n",
    "\n",
    "# Write Random Forest predictions into your MySQL database\n",
    "predDF_rf_clean.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://ht2668-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\",\n",
    "    driver=\"com.mysql.jdbc.Driver\",\n",
    "    dbtable=\"rf_predictions_f1\",   # Table name for Random Forest predictions\n",
    "    user=\"ht2668_gr5069\",\n",
    "    password=config.MYSQL_PASSWORD\n",
    ").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a49258f2-c11a-47e7-a1bb-3208a703e890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "[30 pts] Build two (2) predictive models using MLflow, logging hyperparameters, the model itself, four metrics, and two artifcats. Submit submit your MLflow experiments as part of your assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a671b71-1785-45a8-8f68-4b1b480cb7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow experiment for Linear Regression\n",
    "with mlflow.start_run(run_name=\"LinearRegression_F1\"):\n",
    "\n",
    "    # Train the model\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"milliseconds\")\n",
    "    lrModel = lr.fit(trainDF)\n",
    "\n",
    "    # Predict on test set\n",
    "    predDF_lr = lrModel.transform(testDF)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    evaluator = RegressionEvaluator(labelCol=\"milliseconds\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predDF_lr)\n",
    "    mae = evaluator.setMetricName(\"mae\").evaluate(predDF_lr)\n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predDF_lr)\n",
    "    mse = evaluator.setMetricName(\"mse\").evaluate(predDF_lr)\n",
    "\n",
    "    # Log parameters (no hyperparameters tuned, just record default info)\n",
    "    mlflow.log_param(\"elasticNetParam\", lr.getElasticNetParam())\n",
    "    mlflow.log_param(\"regParam\", lr.getRegParam())\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(lrModel, \"model\")\n",
    "\n",
    "    # Log artifacts (e.g., model summary text)\n",
    "    with open(\"/tmp/lr_model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(lrModel.summary))\n",
    "\n",
    "    mlflow.log_artifact(\"/tmp/lr_model_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7260ff-6a0e-4f69-8e49-9220fe769b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow experiment for Random Forest\n",
    "with mlflow.start_run(run_name=\"RandomForest_F1\"):\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"milliseconds\", numTrees=100)\n",
    "    rfModel = rf.fit(trainDF)\n",
    "\n",
    "    # Predict on test set\n",
    "    predDF_rf = rfModel.transform(testDF)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    evaluator = RegressionEvaluator(labelCol=\"milliseconds\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predDF_rf)\n",
    "    mae = evaluator.setMetricName(\"mae\").evaluate(predDF_rf)\n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predDF_rf)\n",
    "    mse = evaluator.setMetricName(\"mse\").evaluate(predDF_rf)\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"numTrees\", rf.getNumTrees())\n",
    "    mlflow.log_param(\"maxDepth\", rf.getMaxDepth())\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(rfModel, \"model\")\n",
    "\n",
    "    # Log artifact (feature importance)\n",
    "    feature_importances = rfModel.featureImportances\n",
    "    with open(\"/tmp/rf_feature_importances.txt\", \"w\") as f:\n",
    "        f.write(str(feature_importances))\n",
    "\n",
    "    mlflow.log_artifact(\"/tmp/rf_feature_importances.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d525fd69-66c7-4fd4-9c81-f3c8b1bfd0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLflow Experiment Tracking Summary\n",
    "\n",
    "Both models were tracked using MLflow within Databricks:\n",
    "\n",
    "- Linear Regression: `LinearRegression_F1`\n",
    "- Random Forest: `RandomForest_F1`\n",
    "\n",
    "For each model, the following were logged:\n",
    "- Parameters: model-specific hyperparameters\n",
    "- Metrics: RMSE, MAE, R², and MSE\n",
    "- Model: full Spark ML model\n",
    "- Artifact: model summary (LR) and feature importances (RF)\n",
    "\n",
    "See screenshots of the MLflow runs below:\n",
    "- `mlflow_linear_regression_run.png`\n",
    "- `mlflow_random_forest_run.png`\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8858711870097731,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Homework #5: model deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
